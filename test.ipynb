{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import MistralLMClassification\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/anhnh/miniconda3/envs/commonenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ded3b704b743fcadefb4e72aed0fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralLMClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.3 and are newly initialized: ['info_nce_fc.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "encoder = MistralLMClassification.from_pretrained(\"mistralai/Mistral-7B-v0.3\",\n",
    "                                                torch_dtype=torch.float16,\n",
    "                                                token=\"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\",\n",
    "                                                device_map=\"cuda:0\")\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_CLS,\n",
    "                        target_modules=[\"q_proj\", \"v_proj\", \"o_proj\", \"lm_head\"],\n",
    "                        r=16,\n",
    "                        lora_alpha=32,\n",
    "                        lora_dropout=0.1,)\n",
    "                        # modules_to_save=[\"info_nce_fc\"])\n",
    "\n",
    "encoder = get_peft_model(encoder, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 11,599,872 || all params: 7,393,841,152 || trainable%: 0.15688559926476495\n"
     ]
    }
   ],
   "source": [
    "encoder.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl\n",
      "data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl\n",
      "data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl\n",
      "Task_order: [7 3 0 5 4 1 6 2]\n"
     ]
    }
   ],
   "source": [
    "sampler = data_sampler_CFRL(config=config, seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\",\n",
    "                                              token=\"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\",\n",
    "                                              use_fast=False)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "founded in 1961 , the oana groups 41 news agencies of 33 countries seeking to facilitate the free flow of information and cooperation between agencies in the region .\n",
      "Relation between \"oana\" and \"1961\" is\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(sampler.test_data[0][1]['ids'], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(sampler.test_data[0][1]['ids'][58], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haha\n",
      "tensor([1.0000, 2.5000,    inf], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Creating a tensor with some NaN values\n",
    "tensor = torch.tensor([1.0, 2.5, torch.inf], dtype=torch.float16)\n",
    "\n",
    "if torch.isinf(tensor).sum() > 0:\n",
    "    print(\"haha\")\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = LlamaLMClassification.from_pretrained(\"meta-llama/Llama-2-7b-hf\",\n",
    "                                                        torch_dtype=torch.float16,\n",
    "                                                        token=\"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\",\n",
    "                                                        device_map=\"auto\")\n",
    "        peft_config = LoraConfig(task_type=TaskType.SEQ_CLS,\n",
    "                                target_modules=[\"q_proj\", \"v_proj\", \"o_proj\", \"lm_head\"],\n",
    "                                r=16,\n",
    "                                lora_alpha=32,\n",
    "                                lora_dropout=0.1,\n",
    "                                modules_to_save=[\"info_nce_fc\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
